{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Assignment for the CE889 - Neural Networks course\n",
    "\n",
    "## Group #18\n",
    "\n",
    "### Team members:\n",
    "\n",
    "Sudeep Sawant\n",
    "\n",
    "Francisco SÃ¡nchez\n",
    "\n",
    "James Scoon\n",
    "\n",
    "### The first step of the assignment was to import the necessary libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, the training data is split into input and output variables using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_store.drop(columns=[\"Sales\", \"Customers\"]).to_numpy()\n",
    "y = training_store['Sales'].to_numpy()\n",
    "X_test = testing_store.drop(columns=[\"Id\"]).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The input values are then normalised\n",
    "\n",
    "We tried normalising the output, but we noted that it had a worse effect on the learning of the network than leaving the data alone. We believe this is because there are a lot of columns with zeroes, and by normalising it is easier for the network to predict only zeroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputScaler = MinMaxScaler()\n",
    "inputScaler.fit(X)\n",
    "X = inputScaler.transform(X)\n",
    "X_test = inputScaler.transform(X_test)\n",
    "#y = y.reshape(-1, 1)\n",
    "#outputScaler = MinMaxScaler()\n",
    "#outputScaler.fit(y)\n",
    "#y = outputScaler.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To make the data fit the CNN, we reshape it into multiple dimensions\n",
    "\n",
    "The training data is separated into 942 tables, each one corresponding to one consecutive day of the year. These tables have 1115 rows (one for each store) with 27 columns of features.\n",
    "\n",
    "The idea is that the CNN uses a filter to compare similar stores over different days to predict the sales.\n",
    "\n",
    "The test data is only split into 48 days of tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(X, (942, 1115, 27))\n",
    "y = np.reshape(y, (942, 1115))\n",
    "X_test = np.reshape(X_test, (48, 1115, 27))\n",
    "X = np.expand_dims(X, axis = 3)\n",
    "X_test = np.expand_dims(X_test, axis = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see the shape of the data is consistent with what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\t (942, 1115, 27, 1)\n",
      "y:\t (942, 1115)\n",
      "X_test:\t (48, 1115, 27, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X:\\t\", X.shape)\n",
    "print(\"y:\\t\", y.shape)\n",
    "print(\"X_test:\\t\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, the data can be split into training and validation sets\n",
    "\n",
    "The data is split into 75% training data and 25% validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.25, random_state = 42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neural Network is constructed in the following lines\n",
    "\n",
    "We are using a deep convolutional neural network with a convolutional layer that contains 30 10x10 filters, which feeds into another convolutional layer with 25 4x4 filters. This layer is then sent into a 2x2 pooling layer. The result is then flattened and output into a set of 1115 output neurons (one for each store)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(30, 10, input_shape = (1115, 27, 1)))\n",
    "model.add(Conv2D(25, 4, input_shape = (1115, 27, 1)))\n",
    "model.add(MaxPooling2D(pool_size = 2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1115, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we use the Adam optimizer with a learning rate of 0.005 and a decay of 10^-6\n",
    "\n",
    "We also use mean square error as the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.005, decay=1e-6)\n",
    "\n",
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can finally train the network\n",
    "\n",
    "The network is trained for 20 epochs with a batch size of 10. An early stopping criterion is used to stop the network if no progress is made. This criterion also returns the best set of weights that it found while training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 706 samples, validate on 236 samples\n",
      "Epoch 1/20\n",
      "706/706 [==============================] - 133s 189ms/step - loss: 22394384.1841 - accuracy: 0.1742 - val_loss: 18719110.1525 - val_accuracy: 0.4661\n",
      "Epoch 2/20\n",
      "706/706 [==============================] - 132s 187ms/step - loss: 15064798.8244 - accuracy: 0.4490 - val_loss: 13134284.4661 - val_accuracy: 0.2076\n",
      "Epoch 3/20\n",
      "706/706 [==============================] - 128s 181ms/step - loss: 12121371.6813 - accuracy: 0.5241 - val_loss: 11768578.8305 - val_accuracy: 0.5932\n",
      "Epoch 4/20\n",
      "706/706 [==============================] - 127s 179ms/step - loss: 10826760.7295 - accuracy: 0.6487 - val_loss: 10782480.4322 - val_accuracy: 0.6949\n",
      "Epoch 5/20\n",
      "706/706 [==============================] - 126s 179ms/step - loss: 10299392.4235 - accuracy: 0.7153 - val_loss: 10463174.9788 - val_accuracy: 0.7034\n",
      "Epoch 6/20\n",
      "706/706 [==============================] - 124s 175ms/step - loss: 10070239.8839 - accuracy: 0.7054 - val_loss: 10230729.7415 - val_accuracy: 0.7034\n",
      "Epoch 7/20\n",
      "706/706 [==============================] - 124s 176ms/step - loss: 9708342.1686 - accuracy: 0.7139 - val_loss: 9839445.1017 - val_accuracy: 0.7076\n",
      "Epoch 8/20\n",
      "706/706 [==============================] - 124s 176ms/step - loss: 9480379.6749 - accuracy: 0.7295 - val_loss: 9784463.9280 - val_accuracy: 0.7288\n",
      "Epoch 9/20\n",
      "706/706 [==============================] - 124s 176ms/step - loss: 9475035.5751 - accuracy: 0.7195 - val_loss: 9877114.3856 - val_accuracy: 0.7415\n",
      "Epoch 10/20\n",
      "706/706 [==============================] - 125s 177ms/step - loss: 9423623.8470 - accuracy: 0.7365 - val_loss: 9664596.1610 - val_accuracy: 0.7373\n",
      "Epoch 11/20\n",
      "706/706 [==============================] - 124s 176ms/step - loss: 9302002.6827 - accuracy: 0.7479 - val_loss: 9541383.0466 - val_accuracy: 0.7458\n",
      "Epoch 12/20\n",
      "706/706 [==============================] - 124s 176ms/step - loss: 9228641.5283 - accuracy: 0.7422 - val_loss: 9505164.2712 - val_accuracy: 0.7542\n",
      "Epoch 13/20\n",
      "706/706 [==============================] - 124s 176ms/step - loss: 9148886.6232 - accuracy: 0.7436 - val_loss: 9530882.8814 - val_accuracy: 0.7542\n",
      "Epoch 14/20\n",
      "706/706 [==============================] - 124s 176ms/step - loss: 9174888.4873 - accuracy: 0.7507 - val_loss: 9456185.7500 - val_accuracy: 0.7500\n",
      "Epoch 15/20\n",
      "706/706 [==============================] - 124s 176ms/step - loss: 9013418.5368 - accuracy: 0.7436 - val_loss: 9312062.4534 - val_accuracy: 0.7458\n",
      "Epoch 16/20\n",
      "706/706 [==============================] - 126s 178ms/step - loss: 8948824.0857 - accuracy: 0.7465 - val_loss: 9255547.1780 - val_accuracy: 0.7458\n",
      "Epoch 17/20\n",
      "706/706 [==============================] - 126s 178ms/step - loss: 8997730.3952 - accuracy: 0.7507 - val_loss: 9121324.0169 - val_accuracy: 0.7458\n",
      "Epoch 18/20\n",
      "706/706 [==============================] - 125s 178ms/step - loss: 8864474.6742 - accuracy: 0.7507 - val_loss: 9332922.8559 - val_accuracy: 0.7415\n",
      "Epoch 19/20\n",
      "706/706 [==============================] - 124s 176ms/step - loss: 8807675.1006 - accuracy: 0.7550 - val_loss: 9376203.9703 - val_accuracy: 0.7415\n",
      "Epoch 20/20\n",
      "706/706 [==============================] - 125s 176ms/step - loss: 8757253.8237 - accuracy: 0.7564 - val_loss: 9050704.0636 - val_accuracy: 0.7373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f053b989150>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 20, batch_size = 10, validation_data = (X_validation, y_validation), callbacks = [\n",
    "    EarlyStopping(monitor = 'val_loss', min_delta = 1e-3, patience = 3, verbose = 1, mode = 'auto', restore_best_weights = True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracy is somewhat acceptable, at around 76%\n",
    "\n",
    "The next step is to predict and format the output, by turning it into a single column of data (the sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)\n",
    "#y_test = outputScaler.inverse_transform(model.predict(X_test))\n",
    "output = np.reshape(y_test, 48 * 1115)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the last step of the assignment, the results are saved to the submission.csv file. Because the test data had extra rows inserted with an Id of zero, we ignore these rows while exporting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputRows = []\n",
    "for index in range(len(output)):\n",
    "    if testing_store[\"Id\"].iloc[index] == 0:\n",
    "        continue\n",
    "    outputRows.append(output[index])\n",
    "outputFile = open(\"submission.csv\", \"w\")\n",
    "outputFile.write('\"Id\",\"Sales\"\\n')\n",
    "for row in range(len(outputRows)):\n",
    "    outputFile.write(f\"{row + 1},{outputRows[row]}\\n\")\n",
    "outputFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
